/*
 * Copyright 2020 Actyx AG
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
use crate::{
    coll::Coll,
    db::{DbMechanics, DbRecordExt, DB},
    flow::NeedsState,
    machine::{Inputs, Machine},
};
use actyxos_sdk::{
    event::{Event, OffsetMap, Payload},
    event_service::{EventService, Order, Subscription},
};
use anyhow::{anyhow, Result};
use differential_dataflow::ExchangeData;
use futures::{
    future::ready,
    stream::{self, StreamExt},
};
use std::{
    fmt::Write,
    future::Future,
    sync::mpsc::{sync_channel, SyncSender, TrySendError},
    time::{SystemTime, UNIX_EPOCH},
};
use tokio::{
    runtime::Handle,
    time::{interval, Duration},
};
use tracing::{debug, debug_span, error, field, info, info_span, warn};

/// Take care of operating the database, spawning the business logic in a new thread
///
/// The deltas generated by the business logic are transferred to the dB via a
/// [`sync_channel`](https://doc.rust-lang.org/std/sync/mpsc/fn.sync_channel.html).
/// The business logic is run in the context of the Tokio runtime whose handle is
/// supplied as the first argument. This runtime should be using the threaded scheduler
/// to avoid problems when no core thread is running; it can be shared between multiple
/// DB/logic pairs. The differential dataflow created by the [`Machine`](../machine/struct.Machine.html)
/// uses a single-threaded worker, so the runtime’s threads will only be used to run tasks
/// for HTTP communication with the ActyxOS Event Service.
pub fn run_with_db_channel<D, M, Fut, Out, F, N>(
    runtime: Handle,
    db: &mut D,
    name: N,
    logic: F,
) -> Result<()>
where
    M: DbMechanics,
    D: DB<Mechanics = M, Record = Out>,
    F: FnOnce(OffsetMap, SyncSender<(OffsetMap, Vec<(Out, isize)>)>) -> Fut + Send + 'static,
    Fut: Future<Output = Result<()>>,
    Out: ExchangeData + DbRecordExt<M>,
    N: Into<String>,
{
    let name = field::display(name.into());
    let span = info_span!("DB", name);
    let _guard = span.enter();

    info!("starting DB {}", db.to_debug_string());
    let offsets = db.get_offsets()?;
    debug!(events = offsets.size(), "got offsets from DB");

    // limit the machine to run at most two batches ahead of the database
    // (1 in the channel, 1 in hand)
    let (to_db, from_machine) = sync_channel(1);

    std::thread::spawn(move || {
        debug!(name, "starting machine thread");
        // we cannot make the returned Future Send because differential dataflow does not allow that
        runtime.block_on(logic(offsets, to_db)).unwrap();
    });

    loop {
        // will kill this loop if the sender went away
        let (offsets, out) = from_machine.recv()?;
        info!(
            events = offsets.size(),
            deltas = out.len(),
            "received from machine"
        );
        db.advance_offsets(&offsets, out)?;
    }
}

enum TickStream<T> {
    Item(T),
    Stop,
    Tick,
}

/// Use the ActyxOS Event Service to drive a differential dataflow machine
///
/// The supplied machine will be fed with the events matching the given
/// subscriptions, with resulting deltas being sent to the DB via the `to_db`
/// channel.
///
/// The operation proceeds in three phases:
///
///  - replay all events up to the previously committed offset map to get the
///    differential dataflow into the same state where it left off before (this
///    phase is skipped for stateless flows)
///  - ingest all events between the previous offset map and the current present
///    in batches of `events_per_txn`
///  - ingest all future live events in batches of 5sec
///
/// The first non-skipped step can be further limited in number of events by using
/// a [`Flow::new_limited`](../flow/struct.Flow.html#method.new_limited) collection.
/// This is only applicable if it is okay to lose events for the given use-case, for
/// example it may be acceptable to lose the downstream effects of all operational
/// metrics older than two weeks for the purpose of a live dashboard.
pub async fn run_event_machine_on_channel<I, O, R, St: NeedsState, N>(
    mut machine: Machine<I, O, St>,
    subscriptions: Vec<Subscription>,
    mut offsets: OffsetMap,
    to_db: SyncSender<(OffsetMap, Vec<(R, isize)>)>,
    name: N,
    events_per_txn: usize,
) -> Result<()>
where
    I: Inputs<Elem = Event<Payload>>,
    O: ExchangeData,
    R: From<O>,
    N: Into<String>,
{
    let span1 = info_span!("event_machine", name = field::display(name.into()));
    let _guard1 = span1.enter();
    info!("starting with subscriptions {:?}", subscriptions);

    // The plan here is:
    // - get previous offsets O and present P
    // - get all events up to O and ignore the output
    // - get events between O and P and write output in batches of 10'000 events
    // - get events after P and write output every 5sec
    let client = EventService::default();
    let present = client.get_offsets().await?;

    let mut progress: usize = 0;
    let mut errors: usize = 0;

    let feed = |ev: &Event<Payload>,
                machine: &mut Machine<I, O, St>,
                progress: &mut usize,
                errors: &mut usize| match machine.inputs().feed(ev) {
        Ok(_) => {
            *progress += 1;
        }
        Err(err) => {
            *errors += 1;
            error!(
                "Unrecognized event {} @ {:?}: {:?}",
                *progress + *errors,
                ev,
                err
            );
        }
    };

    let send = |offsets: &OffsetMap, deltas: Coll<O, isize>| {
        debug!(
            progress = offsets.size(),
            deltas = deltas.len(),
            "sending batch to DB"
        );
        match to_db.try_send((offsets.clone(), deltas.to_vec())) {
            Ok(_) => Ok(()),
            Err(TrySendError::Full(x)) => {
                info!(deltas = deltas.len(), "back-pressure from DB");
                to_db
                    .send(x)
                    .map_err(|x| anyhow!("cannot send to DB: {}", x))?;
                Ok(())
            }
            Err(_) => Err(anyhow!("channel to DB was closed")),
        }
    };

    // If the machine is stateless, it will just skip all previously seen events.
    // If not, it may say “I need only the last two weeks” or so, otherwise start at the beginning.
    let span = info_span!("up to previous offsets");
    let enter = span.enter();
    if machine.needs_state() {
        let start_at = if let Some(look_back) = machine.look_back() {
            go_back(
                &offsets,
                OffsetMap::empty(),
                look_back,
                &subscriptions,
                &client,
            )
            .await?
        } else {
            OffsetMap::empty()
        };
        info!(
            "getting matches from {} previously ingested events",
            &offsets - &start_at
        );
        let mut events = client
            .query_between(
                start_at,
                offsets.clone(),
                subscriptions.clone(),
                Order::Lamport,
            )
            .await?;
        while let Some(event) = events.next().await {
            feed(&event, &mut machine, &mut progress, &mut errors);
            if progress % events_per_txn == 0 {
                let deltas = machine.drain_deltas();
                info!(progress, deltas = deltas.len(), "got batch");
            }
        }
        let deltas = machine.drain_deltas();
        info!(progress, deltas = deltas.len(), "got last batch");
    } else {
        info!(
            "logic is stateless, skipping {} previously seen events",
            offsets.size()
        );
    }
    drop(enter);
    drop(span);

    let ignored = progress;

    // For a stateless machine we may not need to start at the previous offsets if it
    // tells us that it is only interested “in the last two weeks“, i.e. gaps are okay.
    let span = info_span!("previous to present offsets");
    let enter = span.enter();

    let start_at = if machine.needs_state() {
        offsets.clone()
    } else if let Some(look_back) = machine.look_back() {
        // need to ensure that previous offsets are the minimum to avoid duplicates
        go_back(
            &present,
            offsets.clone(),
            look_back,
            &subscriptions,
            &client,
        )
        .await?
    } else {
        offsets.clone()
    };
    info!(
        "getting matches from {} events to catch up to live data",
        &present - &start_at
    );
    let mut events = client
        .query_between(
            start_at,
            present.clone(),
            subscriptions.clone(),
            Order::Lamport,
        )
        .await?;
    while let Some(event) = events.next().await {
        offsets += &event;
        feed(&event, &mut machine, &mut progress, &mut errors);
        if progress % events_per_txn == 0 {
            let deltas = machine.drain_deltas();
            info!(
                progress = progress - ignored,
                deltas = deltas.len(),
                "got batch"
            );
            send(&offsets, deltas)?;
        }
    }
    let deltas = machine.drain_deltas();
    info!(
        progress = progress - ignored,
        deltas = deltas.len(),
        "got last batch",
    );
    send(&offsets, deltas)?;

    drop(enter);
    drop(span);

    info!("switching to live events");
    let span = info_span!("live events");
    let _guard = span.enter();

    let events = client
        .subscribe_from(present, subscriptions.to_vec())
        .await?
        .map(TickStream::Item)
        .chain(stream::iter(vec![TickStream::Stop]));
    let ticks = interval(Duration::from_secs(5)).map(|_| TickStream::Tick);

    let mut input = stream::select(events, ticks);

    let mut last_reported = progress;
    while let Some(elem) = input.next().await {
        match elem {
            TickStream::Item(event) => {
                offsets += &event;
                feed(&event, &mut machine, &mut progress, &mut errors);
            }
            TickStream::Tick => {
                if progress > last_reported {
                    let deltas = machine.drain_deltas();
                    // will kill this loop if the receiver went away
                    send(&offsets, deltas)?;
                    last_reported = progress;
                }
            }
            TickStream::Stop => {
                warn!("event stream stopped, exiting");
                break;
            }
        }
    }

    Ok(())
}

fn print_duration(d: Duration) -> String {
    let mut res = String::new();
    let mut secs = d.as_secs();
    if secs >= 86400 {
        let days = secs / 86400;
        secs %= 86400;
        write!(res, "{}d", days).unwrap();
    }
    if secs >= 3600 || !res.is_empty() {
        let hours = secs / 3600;
        secs %= 3600;
        write!(res, "{}h", hours).unwrap();
    }
    if secs >= 60 || !res.is_empty() {
        let mins = secs / 60;
        secs %= 60;
        write!(res, "{}m", mins).unwrap();
    }
    write!(res, "{}s", secs).unwrap();
    res
}

async fn go_back(
    offsets: &OffsetMap,
    lower_bound: OffsetMap,
    duration: Duration,
    subscriptions: &[Subscription],
    client: &EventService,
) -> Result<OffsetMap> {
    // the logic is telling us that it only needs events since `look_back` ago, so let’s find those
    info!(
        "determining set of events looking back {}",
        print_duration(duration)
    );
    let mut offsets = offsets.clone();
    let cutoff = (SystemTime::now() - duration)
        .duration_since(UNIX_EPOCH)
        .unwrap()
        .as_micros() as u64;

    let mut events: i32 = 0;
    let span = debug_span!("looking back", events = field::Empty);
    let _ = span.enter();
    client
        .query_between(
            lower_bound,
            offsets.clone(),
            subscriptions.to_vec(),
            Order::LamportReverse,
        )
        .await?
        .take_while(|ev| ready(u64::from(ev.timestamp) > cutoff))
        .for_each(|ev| {
            offsets -= &ev;
            events += 1;
            ready(())
        })
        .await;
    span.record("events", &events);

    Ok(offsets)
}
